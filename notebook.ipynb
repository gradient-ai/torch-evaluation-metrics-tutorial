{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "    self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(8, 32, 3, padding=1)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "    self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "    self.pool4 = nn.MaxPool2d(2)\n",
    "    self.conv5 = nn.Conv2d(32, 128, 3, padding=1)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "    self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(128)\n",
    "    self.pool6 = nn.MaxPool2d(2)\n",
    "    self.conv7 = nn.Conv2d(128, 2, 1)\n",
    "    self.pool7 = nn.AvgPool2d(3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #-------------\n",
    "    # INPUT\n",
    "    #-------------\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    \n",
    "    #-------------\n",
    "    # LAYER 1\n",
    "    #-------------\n",
    "    output_1 = self.conv1(x)\n",
    "    output_1 = F.relu(output_1)\n",
    "    output_1 = self.batchnorm1(output_1)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 2\n",
    "    #-------------\n",
    "    output_2 = self.conv2(output_1)\n",
    "    output_2 = F.relu(output_2)\n",
    "    output_2 = self.pool2(output_2)\n",
    "    output_2 = self.batchnorm2(output_2)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 3\n",
    "    #-------------\n",
    "    output_3 = self.conv3(output_2)\n",
    "    output_3 = F.relu(output_3)\n",
    "    output_3 = self.batchnorm3(output_3)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 4\n",
    "    #-------------\n",
    "    output_4 = self.conv4(output_3)\n",
    "    output_4 = F.relu(output_4)\n",
    "    output_4 = self.pool4(output_4)\n",
    "    output_4 = self.batchnorm4(output_4)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 5\n",
    "    #-------------\n",
    "    output_5 = self.conv5(output_4)\n",
    "    output_5 = F.relu(output_5)\n",
    "    output_5 = self.batchnorm5(output_5)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 6\n",
    "    #-------------\n",
    "    output_6 = self.conv6(output_5)\n",
    "    output_6 = F.relu(output_6)\n",
    "    output_6 = self.pool6(output_6)\n",
    "    output_6 = self.batchnorm6(output_6)\n",
    "\n",
    "    #--------------\n",
    "    # OUTPUT LAYER\n",
    "    #--------------\n",
    "    output_7 = self.conv7(output_6)\n",
    "    output_7 = self.pool7(output_7)\n",
    "    output_7 = output_7.view(-1, 2)\n",
    "\n",
    "    return F.softmax(output_7, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding the right combination"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(network, training_set, batch_size, optimizer, loss_function):\n",
    "  \"\"\"\n",
    "  This function optimizes the convnet weights\n",
    "  \"\"\"\n",
    "  #  creating list to hold loss per batch\n",
    "  loss_per_batch = []\n",
    "\n",
    "  #  defining dataloader\n",
    "  train_loader = DataLoader(training_set, batch_size)\n",
    "\n",
    "  #  iterating through batches\n",
    "  print('training...')\n",
    "  for images, labels in tqdm(train_loader):\n",
    "    #---------------------------\n",
    "    #  sending images to device\n",
    "    #---------------------------\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    #-----------------------------\n",
    "    #  zeroing optimizer gradients\n",
    "    #-----------------------------\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #-----------------------\n",
    "    #  classifying instances\n",
    "    #-----------------------\n",
    "    classifications = network(images)\n",
    "\n",
    "    #---------------------------------------------------\n",
    "    #  computing loss/how wrong our classifications are\n",
    "    #---------------------------------------------------\n",
    "    loss = loss_function(classifications, labels)\n",
    "    loss_per_batch.append(loss.item())\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    #  computing gradients/the direction that fits our objective\n",
    "    #------------------------------------------------------------\n",
    "    loss.backward()\n",
    "\n",
    "    #---------------------------------------------------\n",
    "    #  optimizing weights/slightly adjusting parameters\n",
    "    #---------------------------------------------------\n",
    "    optimizer.step()\n",
    "  print('all done!')\n",
    "\n",
    "  return loss_per_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proper generalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def validate(network, validation_set, batch_size, loss_function):\n",
    "  \"\"\"\n",
    "  This function validates convnet parameter optimizations\n",
    "  \"\"\"\n",
    "  #  creating a list to hold loss per batch\n",
    "  loss_per_batch = []\n",
    "\n",
    "  #  defining model state\n",
    "  network.eval()\n",
    "\n",
    "  #  defining dataloader\n",
    "  val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "  print('validating...')\n",
    "  #  preventing gradient calculations since we will not be optimizing\n",
    "  with torch.no_grad():\n",
    "    #  iterating through batches\n",
    "    for images, labels in tqdm(val_loader):\n",
    "      #--------------------------------------\n",
    "      #  sending images and labels to device\n",
    "      #--------------------------------------\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      #--------------------------\n",
    "      #  making classsifications\n",
    "      #--------------------------\n",
    "      classifications = network(images)\n",
    "\n",
    "      #-----------------\n",
    "      #  computing loss\n",
    "      #-----------------\n",
    "      loss = loss_function(classifications, labels)\n",
    "      loss_per_batch.append(loss.item())\n",
    "\n",
    "  print('all done!')\n",
    "\n",
    "  return loss_per_batch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Measuring performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def accuracy(network, dataset):\n",
    "  \"\"\"\n",
    "  This function computes accuracy\n",
    "  \"\"\"\n",
    "  #  setting model state\n",
    "  network.eval()\n",
    "  \n",
    "  #  instantiating counters\n",
    "  total_correct = 0\n",
    "  total_instances = 0\n",
    "\n",
    "  #  creating dataloader\n",
    "  dataloader = DataLoader(dataset, 64)\n",
    "\n",
    "  #  iterating through batches\n",
    "  with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader):\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "      #-------------------------------------------------------------------------\n",
    "      #  making classifications and deriving indices of maximum value via argmax\n",
    "      #-------------------------------------------------------------------------\n",
    "      classifications = torch.argmax(network(images), dim=1)\n",
    "\n",
    "      #--------------------------------------------------\n",
    "      #  comparing indicies of maximum values and labels\n",
    "      #--------------------------------------------------\n",
    "      correct_predictions = sum(classifications==labels).item()\n",
    "\n",
    "      #------------------------\n",
    "      #  incrementing counters\n",
    "      #------------------------\n",
    "      total_correct+=correct_predictions\n",
    "      total_instances+=len(images)\n",
    "  return round(total_correct/total_instances, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connecting the Pieces\n",
    "\n",
    "### Loading the pieces"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convnet architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "    self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(8, 32, 3, padding=1)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "    self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "    self.pool4 = nn.MaxPool2d(2)\n",
    "    self.conv5 = nn.Conv2d(32, 128, 3, padding=1)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "    self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(128)\n",
    "    self.pool6 = nn.MaxPool2d(2)\n",
    "    self.conv7 = nn.Conv2d(128, 10, 1)\n",
    "    self.pool7 = nn.AvgPool2d(3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #-------------\n",
    "    # INPUT\n",
    "    #-------------\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    \n",
    "    #-------------\n",
    "    # LAYER 1\n",
    "    #-------------\n",
    "    output_1 = self.conv1(x)\n",
    "    output_1 = F.relu(output_1)\n",
    "    output_1 = self.batchnorm1(output_1)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 2\n",
    "    #-------------\n",
    "    output_2 = self.conv2(output_1)\n",
    "    output_2 = F.relu(output_2)\n",
    "    output_2 = self.pool2(output_2)\n",
    "    output_2 = self.batchnorm2(output_2)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 3\n",
    "    #-------------\n",
    "    output_3 = self.conv3(output_2)\n",
    "    output_3 = F.relu(output_3)\n",
    "    output_3 = self.batchnorm3(output_3)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 4\n",
    "    #-------------\n",
    "    output_4 = self.conv4(output_3)\n",
    "    output_4 = F.relu(output_4)\n",
    "    output_4 = self.pool4(output_4)\n",
    "    output_4 = self.batchnorm4(output_4)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 5\n",
    "    #-------------\n",
    "    output_5 = self.conv5(output_4)\n",
    "    output_5 = F.relu(output_5)\n",
    "    output_5 = self.batchnorm5(output_5)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 6\n",
    "    #-------------\n",
    "    output_6 = self.conv6(output_5)\n",
    "    output_6 = F.relu(output_6)\n",
    "    output_6 = self.pool6(output_6)\n",
    "    output_6 = self.batchnorm6(output_6)\n",
    "\n",
    "    #--------------\n",
    "    # OUTPUT LAYER\n",
    "    #--------------\n",
    "    output_7 = self.conv7(output_6)\n",
    "    output_7 = self.pool7(output_7)\n",
    "    output_7 = output_7.view(-1, 10)\n",
    "\n",
    "    return F.softmax(output_7, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joining process"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  instantiating model\n",
    "model = ConvNet()\n",
    "#  defining optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "#  training/optimizing parameters\n",
    "training_losses = train(network=model.to('cuda'), training_set=training_set, \n",
    "                        batch_size=64, optimizer=optimizer, \n",
    "                        loss_function=nn.CrossEntropyLoss())\n",
    "                        \n",
    "#  validating optimizations                        \n",
    "validation_losses = validate(network=model, validation_set=validation_set, \n",
    "                             batch_size=64, loss_function=nn.CrossEntropyLoss())\n",
    "\n",
    "#  deriving model accuracy on the traininig set\n",
    "training_accuracy = accuracy(model, training_set)\n",
    "print(f'training accuracy: {training_accuracy}')\n",
    "\n",
    "#  deriving model accuracy on the validation set\n",
    "validation_accuracy = accuracy(model, validation_set)\n",
    "print(f'validation accuracy: {validation_accuracy}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalNeuralNet():\n",
    "  def __init__(self, network):\n",
    "    self.network = network.to(device)\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  defining accuracy function\n",
    "    def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      return round(total_correct/total_instances, 3)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      for images, labels in tqdm(train_loader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        self.optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy = accuracy(self.network, train_loader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      self.network.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = self.network(images)\n",
    "          #  computing loss\n",
    "          val_loss = loss_function(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy = accuracy(self.network, val_loader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}\\n')\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def predict(self, x):\n",
    "    return self.network(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalNeuralNet(ConvNet())\n",
    "\n",
    "log_dict = model.train(nn.CrossEntropyLoss(), epochs=10, batch_size=64, \n",
    "                       training_set=training_set, validation_set=validation_set)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}